{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "steps:\n",
        "1.   get the csv with all the messages and the coders' ratings\n",
        "2.   extract the messages\n",
        "3.   calculate ground-truth ratings\n",
        "4.   generate the personas\n",
        "5.   get the desired set of messages\n",
        "6.   loop through everything and get the desired outputs\n",
        "7.   store that\n",
        "8.   calculate the difference between that and the ground-truth ratings\n",
        "\n",
        "NEED TO DO after getting coders' ratings:\n",
        "3, 6, 7"
      ],
      "metadata": {
        "id": "hAw8MfkCpDNQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Methods here"
      ],
      "metadata": {
        "id": "Js2KwEwJoUjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_api_key = \"\" # from PASCL's GPT-4 account"
      ],
      "metadata": {
        "id": "SlaVQlyGX2rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3pttQz6g1BjC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5218b2f-cada-4609-930d-d063a628bfe0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.35.7)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.7.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.18.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFm4mLOk0X1m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db77e605-f931-4ceb-d7c4-2c861ac68431",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import os\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = my_api_key\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "client = OpenAI()"
      ],
      "metadata": {
        "id": "ZSJB5eKc9_89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract the messages, sort by reactivity\n",
        "df_sorted = extract_messages(10, 0) # 10 messages, most effect on well-being"
      ],
      "metadata": {
        "id": "orq7_MoHZAJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate ground-truth ratings"
      ],
      "metadata": {
        "id": "5inCgE5E5M3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate the personas\n",
        "numbers = [1, 10, 20, 50, 70, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "# trying out different numbers of participants to see which number will best reflect the real-world\n",
        "averages = findBest(numbers)\n",
        "#print(numbers)\n",
        "#print(averages)\n",
        "plot(numbers, averages)"
      ],
      "metadata": {
        "id": "ls3dgy5cSm03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# say, n = 100\n",
        "participants, attributes = generate_participants(100)\n",
        "personas = generatePersonas(participants)"
      ],
      "metadata": {
        "id": "kWscn7ScAW7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calls(messages, personas)\n",
        "# get the desired outputs and store them\n",
        "# calculate precision and recall"
      ],
      "metadata": {
        "id": "45KIOGYAQXX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Methods here"
      ],
      "metadata": {
        "id": "S19K4iTsoM6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get the messages and sort them by reactivity\n",
        "def extract_messages(n, type):\n",
        "  df = pd.read_csv(\"/content/drive/MyDrive/css summer 2024/stanford/rating tweets/Rating Tweets - Sheet1.csv\")\n",
        "  df = df[df['Remove'] != 1]\n",
        "  df = df.drop(columns=['Remove'])\n",
        "  messages = df.iloc[:,3]\n",
        "  df['reaction'] = abs(df.iloc[:, [4, 5, 6]].mean(axis=1) - 50) # how much of an effect the post has on well-being\n",
        "  df_sorted = df.sort_values(by='reaction') # 0 --> 50\n",
        "  if type == 0: # n posts which provoke the strongest reaction\n",
        "    return df_sorted.iloc[-n:, [3, -1]]\n",
        "  elif type == 1: # n posts which provoke the weakest reaction\n",
        "    return df_sorted.iloc[0:n, [3, -1]]\n",
        "  elif type == 2: # random n posts\n",
        "    return df_sorted.sample(n).iloc[:, [3, -1]]"
      ],
      "metadata": {
        "id": "G-XKp5c_4gwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using my spreadsheet to get the attributes and real-world weights and then generate the participants\n",
        "def generate_participants(n):\n",
        "    df = pd.read_csv('/content/drive/MyDrive/css summer 2024/stanford/Fictional Participants - Sheet2.csv')\n",
        "    attributes = {\n",
        "        'age_group':\n",
        "        {'categories': df.iloc[:, 0].dropna().tolist(), 'weights': df.iloc[:, 1].dropna().tolist()},\n",
        "        'marital_status':\n",
        "        {'categories': df.iloc[:, 2].dropna().tolist(), 'weights': df.iloc[:, 3].dropna().tolist()},\n",
        "        'race':\n",
        "        {'categories': df.iloc[:, 4].dropna().tolist(), 'weights': df.iloc[:, 5].dropna().tolist()},\n",
        "        'sex':\n",
        "        {'categories': df.iloc[:, 6].dropna().tolist(), 'weights': df.iloc[:, 7].dropna().tolist()},\n",
        "        'education':\n",
        "        {'categories': df.iloc[:, 8].dropna().tolist(), 'weights': df.iloc[:, 9].dropna().tolist()},\n",
        "        'income':\n",
        "        {'categories': df.iloc[:, 10].dropna().tolist(), 'weights': df.iloc[:, 11].dropna().tolist()},\n",
        "        'mental_health':\n",
        "        {'categories': df.iloc[:, 12].dropna().tolist(), 'weights': df.iloc[:, 13].dropna().tolist()},\n",
        "        'political_affiliation':\n",
        "        {'categories': df.iloc[:, 14].dropna().tolist(), 'weights': df.iloc[:, 15].dropna().tolist()}\n",
        "    }\n",
        "    data = {}\n",
        "    for attribute, info in attributes.items():\n",
        "        data[attribute] = np.random.choice(info['categories'], size=n, p=info['weights'])\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    return df, attributes"
      ],
      "metadata": {
        "id": "hCCoLVb1O7sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to get the distributions of each attribute and see how closely they match real-world data\n",
        "def verify_dataset(df, attributes):\n",
        "    total_samples = len(df)\n",
        "    verification_results = {}\n",
        "    total_deviations = []\n",
        "\n",
        "    for column, info in attributes.items():\n",
        "        observed_counts = df[column].value_counts().reindex(info['categories'], fill_value=0)\n",
        "        observed_percentages = observed_counts / total_samples\n",
        "        original_weights = pd.Series(info['weights'], index=info['categories'])\n",
        "\n",
        "        deviations = observed_percentages - original_weights\n",
        "        total_deviations.extend(deviations.abs().tolist())\n",
        "\n",
        "        verification_results[column] = {\n",
        "            'observed_percentages': observed_percentages,\n",
        "            'original_weights': original_weights,\n",
        "            'deviations': deviations,\n",
        "            'average_deviation': deviations.abs().mean()\n",
        "        }\n",
        "\n",
        "    overallAverageDeviation = np.mean(total_deviations)\n",
        "    return verification_results, overallAverageDeviation"
      ],
      "metadata": {
        "id": "n6kZJbckYupl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the number of participants which will most-closely mirror real-world data\n",
        "def findBest(numbers):\n",
        "  averages = []\n",
        "  for num in numbers:\n",
        "    participants, attributes = generate_participants(num)\n",
        "    verification_results, overallAverageDeviation = verify_dataset(participants, attributes)\n",
        "    averages.append(overallAverageDeviation)\n",
        "  return averages"
      ],
      "metadata": {
        "id": "Fhv_mjxvkz_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot two lists\n",
        "def plot(a, b):\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.plot(a, b)\n",
        "  plt.xlabel('Number of Participants')\n",
        "  plt.ylabel('Average Deviation')\n",
        "  plt.title('Average Deviation vs. Number of Participants')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "M5jRhHFH597t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the system call string given the attributes of a persona\n",
        "def generatePersonas(df):\n",
        "  personas = []\n",
        "  for i in range(len(df)):\n",
        "    age = df.iloc[i, 0]\n",
        "    marital_status = df.iloc[i, 1]\n",
        "    race = df.iloc[i, 2]\n",
        "    sex = df.iloc[i, 3]\n",
        "    education = df.iloc[i, 4]\n",
        "    income = df.iloc[i, 5]\n",
        "    mental_health = df.iloc[i, 6]\n",
        "    political_affiliation = df.iloc[i, 7]\n",
        "    persona = f\"You are a {age} year-old, {marital_status}, {race} {sex} with {education} making {income} per year, living in the United States. You have {mental_health} and your political affiliation is {political_affiliation}.\"\n",
        "    personas.append(persona)\n",
        "  return personas"
      ],
      "metadata": {
        "id": "ZbP3eAD7DMps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# just an API call\n",
        "def api_call(client, system_content, user_content):\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": system_content},\n",
        "        {\"role\": \"user\", \"content\": user_content}\n",
        "    ]\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "cgKmXO1XXPxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# given the set of messages and personas, making the calls\n",
        "def calls(messages, personas):\n",
        "  user_content = \"\"\"\n",
        "  You are presented with a social media post. On a scale from 0 to 100, please indicate how you would feel,\n",
        "  as a social media user, after seeing the social media post. Specifically, answer these questions:\n",
        "  (1) if you would feel satisfied with yourself,\n",
        "  (2) if you have a positive attitude towards yourself, and\n",
        "  (3) if you would feel that you have a number of good qualities.\n",
        "  Please just output three numbers, separated by a comma and a space, and nothing else.\n",
        "  An example format is: \"50, 50, 50\" and nothing else.\n",
        "\n",
        "  SOCIAL MEDIA POST:\n",
        "  \"\"\"\n",
        "  for message in messages:\n",
        "    for persona in personas:\n",
        "      user_content += message\n",
        "      system_content = persona\n",
        "      response = api_call(client, system_content, user_content)\n",
        "      print(response)\n",
        "      print(\"--------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "swjchfaXJtUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "def calculate_metrics(ground_truth, predicted):\n",
        "    accuracy = accuracy_score(ground_truth, predicted)\n",
        "    precision = precision_score(ground_truth, predicted, average='macro')\n",
        "    recall = recall_score(ground_truth, predicted, average='macro')\n",
        "    f1 = f1_score(ground_truth, predicted, average='macro')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1\n",
        "    }"
      ],
      "metadata": {
        "id": "prwLfB_3a2Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Don't need to run this"
      ],
      "metadata": {
        "id": "1T799Kl3oj7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# testing to make sure that it works\n",
        "user_content = \"What is the meaning of life?\"\n",
        "system_content = \"You are a helpful assistant.\"\n",
        "\n",
        "response = api_call(client, system_content, user_content)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "gp3k4cyQXgEe",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}